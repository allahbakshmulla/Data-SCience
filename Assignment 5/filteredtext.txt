 Unit 5 - Machine Learning Algorithms Syllabus: Classification Regression algorithms- Naïve Bayes, K-Nearest Neighbors, logistic regression, support vector machines (SVM), decision trees, random forest, Classification Errors, Analysis Time Series- Linear Systems Analysis, Nonlinear Dynamics, Rule Induction, Neural Networks- Learning And Generalization, Overview Deep Learning. 5.1 Classification Regression algorithms- Naïve Bayes, K-Nearest Neighbors, logistic regression, support vector machines (SVM), decision trees, random forest Classification vs Regression Supervised learning furthered categorized classification regression algorithms. Classification model identifies category object belongs whereas regression model predicts continuous output. Sometimes ambiguous line classification algorithms regression algorithms. Many algorithms used classification regression, classification regression model threshold applied. When number higher threshold classified true lower classified false. 1. Naive Bayes Fig. 5.1 naive bayes Naive Bayes based Bayes’ Theorem — approach calculate conditional probability based prior knowledge, naive assumption feature independent other. The biggest advantage Naive Bayes that, machine learning algorithms rely large amount training data, performs relatively well even training data size small. Gaussian Naive Bayes type Naive Bayes classifier follows normal distribution. sklearn.naive_bayes import GaussianNB gnb = KNeighborsClassifier() gnb.fit(X_train, y_train) y_pred = gnb.predict(X_test) 2. K-Nearest Neighbour (KNN) Fig. 5.2 KNN You think k nearest neighbour algorithm representing data point n dimensional space — defined n features. And calculates distance one point another, assign label unobserved data based labels nearest observed data points. KNN also used building recommendation system. sklearn.neighbors import KNeighborsClassifier knn = KNeighborsClassifier() knn.fit(X_train, y_train) y_pred = knn.predict(X_test) 3. Logistic Regression Fig. 5.3 Logistic regression Logistics regression uses sigmoid function return probability label. It widely used classification problem binary — true false, win lose, positive negative. The sigmoid function generates probability output. By comparing probability pre-defined threshold, object assigned label accordingly. Below code snippet default logistic regression sklearn.linear_model import LogisticRegression reg = LogisticRegression() reg.fit(X_train, y_train) y_pred = reg.predict(X_test) 4. Support Vector Machine (SVM) Fig. 5.4 Support Vector Machine Support vector machine finds best way classify data based position relation border positive class negative class. This border known hyperplane maximize distance data points different classes. Similar decision tree random forest, support vector machine used classification regression, SVC (support vector classifier) classification problem. sklearn.svm import SVC svc = SVC() svc.fit(X_train, y_train) y_pred = svc.predict(X_test) 5. Decision Tree Fig. 5.5 Decision tree Decision tree builds tree branches hierarchy approach branch considered if-else statement. The branches develop partitioning dataset subsets based important features. Final classification happens leaves decision tree. sklearn.tree import DecisionTreeClassifier dtc = DecisionTreeClassifier() dtc.fit(X_train, y_train) y_pred = dtc.predict(X_test) 6. Random Forest Fig. 5.6 Random forest As name suggest, random forest collection decision trees. It common type ensemble methods aggregate results multiple predictors. Random forest additionally utilizes bagging technique allows tree trained random sampling original dataset takes majority vote trees. Compared decision tree, better generalization less interpretable, layers added model. sklearn.ensemble import RandomForestClassifier rfc = RandomForestClassifier() rfc.fit(X_train, y_train) y_pred = rfc.predict(X_test)